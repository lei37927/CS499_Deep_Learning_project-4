{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the spam data from the csv file\n",
    "D = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'make', 'address', 'all', 'num3d', 'our', 'over',\n",
       "       'remove', 'internet', 'order', 'mail', 'receive', 'will', 'people',\n",
       "       'report', 'addresses', 'free', 'business', 'email', 'you', 'credit',\n",
       "       'your', 'font', 'num000', 'money', 'hp', 'hpl', 'george', 'num650',\n",
       "       'lab', 'labs', 'telnet', 'num857', 'data', 'num415', 'num85',\n",
       "       'technology', 'num1999', 'parts', 'pm', 'direct', 'cs', 'meeting',\n",
       "       'original', 'project', 're', 'edu', 'table', 'conference',\n",
       "       'charSemicolon', 'charRoundbracket', 'charSquarebracket',\n",
       "       'charExclamation', 'charDollar', 'charHash', 'capitalAve',\n",
       "       'capitalLong', 'capitalTotal', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam = D.columns\n",
    "nam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          2\n",
       "2          3\n",
       "3          4\n",
       "4          5\n",
       "        ... \n",
       "4596    4597\n",
       "4597    4598\n",
       "4598    4599\n",
       "4599    4600\n",
       "4600    4601\n",
       "Name: Unnamed: 0, Length: 4601, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data cleaning\n",
    "D.pop(nam[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          spam\n",
       "1          spam\n",
       "2          spam\n",
       "3          spam\n",
       "4          spam\n",
       "         ...   \n",
       "4596    nonspam\n",
       "4597    nonspam\n",
       "4598    nonspam\n",
       "4599    nonspam\n",
       "4600    nonspam\n",
       "Name: type, Length: 4601, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = D.pop('type')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform the categorical data into numerica type\n",
    "target = pd.Series(np.array(target == 'spam').astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "4596    0\n",
       "4597    0\n",
       "4598    0\n",
       "4599    0\n",
       "4600    0\n",
       "Length: 4601, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(target == 'spam').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data normalization process \n",
    "normalized_D = D.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data into two parts with 80% training set and 20% testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalized_D, target, \n",
    "                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>num3d</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>remove</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>...</th>\n",
       "      <th>conference</th>\n",
       "      <th>charSemicolon</th>\n",
       "      <th>charRoundbracket</th>\n",
       "      <th>charSquarebracket</th>\n",
       "      <th>charExclamation</th>\n",
       "      <th>charDollar</th>\n",
       "      <th>charHash</th>\n",
       "      <th>capitalAve</th>\n",
       "      <th>capitalLong</th>\n",
       "      <th>capitalTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2731</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.665898</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.091140</td>\n",
       "      <td>-0.231810</td>\n",
       "      <td>-0.239640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>1.476158</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.035319</td>\n",
       "      <td>-0.118914</td>\n",
       "      <td>-0.279225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1483</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>1.011459</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.118626</td>\n",
       "      <td>-0.242073</td>\n",
       "      <td>-0.401281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3738</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>0.489780</td>\n",
       "      <td>0.295842</td>\n",
       "      <td>0.310965</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>0.141409</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.063277</td>\n",
       "      <td>0.112009</td>\n",
       "      <td>-0.125831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3994</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.114938</td>\n",
       "      <td>-0.236941</td>\n",
       "      <td>-0.439217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3062</td>\n",
       "      <td>0.214351</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.137563</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.323860</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>0.610196</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>0.133194</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.085971</td>\n",
       "      <td>0.142799</td>\n",
       "      <td>-0.064804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.851723</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.859224</td>\n",
       "      <td>0.270638</td>\n",
       "      <td>0.142546</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>1.552055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.570211</td>\n",
       "      <td>1.844328</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.029903</td>\n",
       "      <td>1.008954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.116356</td>\n",
       "      <td>-0.252336</td>\n",
       "      <td>-0.457360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2758</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.107165</td>\n",
       "      <td>0.612751</td>\n",
       "      <td>-0.226918</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>0.288290</td>\n",
       "      <td>-0.091046</td>\n",
       "      <td>-0.190757</td>\n",
       "      <td>-0.254484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2272</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.732336</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>0.681959</td>\n",
       "      <td>-0.070652</td>\n",
       "      <td>-0.231810</td>\n",
       "      <td>-0.369942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          make   address       all   num3d       our      over    remove  \\\n",
       "2731 -0.342434 -0.165072 -0.556761 -0.0469  0.665898 -0.350266 -0.291794   \n",
       "1189 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "1483 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "3738 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314  0.489780  0.295842   \n",
       "3994 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "...        ...       ...       ...     ...       ...       ...       ...   \n",
       "3062  0.214351 -0.165072  0.137563 -0.0469  0.323860 -0.350266 -0.291794   \n",
       "734  -0.342434 -0.165072  0.851723 -0.0469  0.859224  0.270638  0.142546   \n",
       "1944 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "2758 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "2272 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "\n",
       "      internet     order      mail  ...  conference  charSemicolon  \\\n",
       "2731 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "1189 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "1483 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "3738  0.310965 -0.323302 -0.371364  ...   -0.111546       0.141409   \n",
       "3994 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "...        ...       ...       ...  ...         ...            ...   \n",
       "3062  0.610196 -0.323302 -0.371364  ...   -0.111546       0.133194   \n",
       "734  -0.262562 -0.323302  1.552055  ...   -0.111546      -0.158453   \n",
       "1944 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "2758 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "2272 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "\n",
       "      charRoundbracket  charSquarebracket  charExclamation  charDollar  \\\n",
       "2731         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "1189         -0.514307          -0.155198         1.476158   -0.308355   \n",
       "1483         -0.514307          -0.155198         1.011459   -0.308355   \n",
       "3738         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "3994         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "...                ...                ...              ...         ...   \n",
       "3062          0.014684          -0.155198        -0.329912   -0.308355   \n",
       "734          -0.514307          -0.155198         0.010948    0.570211   \n",
       "1944         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "2758          0.107165           0.612751        -0.226918   -0.308355   \n",
       "2272          0.732336          -0.155198        -0.329912   -0.308355   \n",
       "\n",
       "      charHash  capitalAve  capitalLong  capitalTotal  \n",
       "2731 -0.103048   -0.091140    -0.231810     -0.239640  \n",
       "1189 -0.103048   -0.035319    -0.118914     -0.279225  \n",
       "1483 -0.103048   -0.118626    -0.242073     -0.401281  \n",
       "3738 -0.103048   -0.063277     0.112009     -0.125831  \n",
       "3994 -0.103048   -0.114938    -0.236941     -0.439217  \n",
       "...        ...         ...          ...           ...  \n",
       "3062 -0.103048   -0.085971     0.142799     -0.064804  \n",
       "734   1.844328    0.105008     0.029903      1.008954  \n",
       "1944 -0.103048   -0.116356    -0.252336     -0.457360  \n",
       "2758  0.288290   -0.091046    -0.190757     -0.254484  \n",
       "2272  0.681959   -0.070652    -0.231810     -0.369942  \n",
       "\n",
       "[921 rows x 57 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the training set further into two parts, 60% as training set, 40% as validation set.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>num3d</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>remove</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>...</th>\n",
       "      <th>conference</th>\n",
       "      <th>charSemicolon</th>\n",
       "      <th>charRoundbracket</th>\n",
       "      <th>charSquarebracket</th>\n",
       "      <th>charExclamation</th>\n",
       "      <th>charDollar</th>\n",
       "      <th>charHash</th>\n",
       "      <th>capitalAve</th>\n",
       "      <th>capitalLong</th>\n",
       "      <th>capitalTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4241</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.457360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3629</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.132116</td>\n",
       "      <td>-0.262599</td>\n",
       "      <td>-0.459010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>3.897738</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.079604</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>-0.427671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2161</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.812048</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.514307</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.123669</td>\n",
       "      <td>-0.247205</td>\n",
       "      <td>-0.381488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>0.116095</td>\n",
       "      <td>-0.056581</td>\n",
       "      <td>0.018536</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>3.338050</td>\n",
       "      <td>0.078467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>0.240338</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.142316</td>\n",
       "      <td>0.313962</td>\n",
       "      <td>0.134550</td>\n",
       "      <td>-0.078375</td>\n",
       "      <td>0.106877</td>\n",
       "      <td>0.139722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2374</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.040978</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>-0.111090</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.161927</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.112196</td>\n",
       "      <td>-0.211283</td>\n",
       "      <td>0.039108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2098</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.160005</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>0.427959</td>\n",
       "      <td>0.380209</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>0.035654</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.333044</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.127924</td>\n",
       "      <td>-0.216415</td>\n",
       "      <td>-0.033465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4411</td>\n",
       "      <td>-0.342434</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>-0.556761</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.464314</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>0.776482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.055601</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.101668</td>\n",
       "      <td>-0.190757</td>\n",
       "      <td>-0.369942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4523</td>\n",
       "      <td>1.033152</td>\n",
       "      <td>-0.165072</td>\n",
       "      <td>0.276427</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.152019</td>\n",
       "      <td>-0.350266</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.262562</td>\n",
       "      <td>-0.323302</td>\n",
       "      <td>-0.371364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>-0.366337</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>-0.329912</td>\n",
       "      <td>-0.308355</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.100596</td>\n",
       "      <td>-0.252336</td>\n",
       "      <td>-0.444165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>1.033152</td>\n",
       "      <td>0.191397</td>\n",
       "      <td>0.197076</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.181761</td>\n",
       "      <td>0.051495</td>\n",
       "      <td>-0.291794</td>\n",
       "      <td>-0.088010</td>\n",
       "      <td>1.758643</td>\n",
       "      <td>0.590345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111546</td>\n",
       "      <td>-0.158453</td>\n",
       "      <td>-0.233165</td>\n",
       "      <td>-0.155198</td>\n",
       "      <td>0.207127</td>\n",
       "      <td>2.071094</td>\n",
       "      <td>0.192785</td>\n",
       "      <td>0.029707</td>\n",
       "      <td>0.517407</td>\n",
       "      <td>3.135028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1472 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          make   address       all   num3d       our      over    remove  \\\n",
       "4241 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "3629 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "476  -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "2161 -0.342434 -0.165072  0.812048 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "413   0.116095 -0.056581  0.018536 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "...        ...       ...       ...     ...       ...       ...       ...   \n",
       "2374 -0.342434 -0.165072 -0.040978 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "2098 -0.342434 -0.165072 -0.160005 -0.0469  0.427959  0.380209 -0.291794   \n",
       "4411 -0.342434 -0.165072 -0.556761 -0.0469 -0.464314 -0.350266 -0.291794   \n",
       "4523  1.033152 -0.165072  0.276427 -0.0469 -0.152019 -0.350266 -0.291794   \n",
       "614   1.033152  0.191397  0.197076 -0.0469 -0.181761  0.051495 -0.291794   \n",
       "\n",
       "      internet     order      mail  ...  conference  charSemicolon  \\\n",
       "4241 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "3629 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "476  -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "2161 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.158453   \n",
       "413  -0.262562  3.338050  0.078467  ...   -0.111546      -0.158453   \n",
       "...        ...       ...       ...  ...         ...            ...   \n",
       "2374 -0.262562 -0.323302 -0.371364  ...   -0.111546      -0.010576   \n",
       "2098 -0.262562  0.035654 -0.371364  ...   -0.111546      -0.158453   \n",
       "4411 -0.262562 -0.323302  0.776482  ...   -0.111546      -0.158453   \n",
       "4523 -0.262562 -0.323302 -0.371364  ...   -0.111546       0.005855   \n",
       "614  -0.088010  1.758643  0.590345  ...   -0.111546      -0.158453   \n",
       "\n",
       "      charRoundbracket  charSquarebracket  charExclamation  charDollar  \\\n",
       "4241         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "3629         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "476          -0.514307          -0.155198         3.897738   -0.308355   \n",
       "2161         -0.514307          -0.155198        -0.329912   -0.308355   \n",
       "413           0.240338          -0.155198        -0.142316    0.313962   \n",
       "...                ...                ...              ...         ...   \n",
       "2374         -0.111090          -0.155198        -0.329912   -0.161927   \n",
       "2098         -0.333044          -0.155198        -0.329912   -0.308355   \n",
       "4411         -0.055601          -0.155198        -0.329912   -0.308355   \n",
       "4523         -0.366337          -0.155198        -0.329912   -0.308355   \n",
       "614          -0.233165          -0.155198         0.207127    2.071094   \n",
       "\n",
       "      charHash  capitalAve  capitalLong  capitalTotal  \n",
       "4241 -0.103048   -0.132116    -0.262599     -0.457360  \n",
       "3629 -0.103048   -0.132116    -0.262599     -0.459010  \n",
       "476  -0.103048   -0.079604    -0.211283     -0.427671  \n",
       "2161 -0.103048   -0.123669    -0.247205     -0.381488  \n",
       "413   0.134550   -0.078375     0.106877      0.139722  \n",
       "...        ...         ...          ...           ...  \n",
       "2374 -0.103048   -0.112196    -0.211283      0.039108  \n",
       "2098 -0.103048   -0.127924    -0.216415     -0.033465  \n",
       "4411 -0.103048   -0.101668    -0.190757     -0.369942  \n",
       "4523 -0.103048   -0.100596    -0.252336     -0.444165  \n",
       "614   0.192785    0.029707     0.517407      3.135028  \n",
       "\n",
       "[1472 rows x 57 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.values, y_train.values))\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((x_val.values, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model with one layer and 10 neurons\n",
    "def get_compiled_model10():\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer= 'sgd',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model with one layer and 100 neurons\n",
    "def get_compiled_model100():\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='sgd',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model with one layer and 1000 neurons\n",
    "def get_compiled_model1000():\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='sgd',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(len(x_train)).batch(32)\n",
    "val_dataset = dataset_val.shuffle(len(x_val)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model.\n",
    "model10 = get_compiled_model10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 69 steps, validate for 46 steps\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.7739 - accuracy: 0.6132 - val_loss: 0.6104 - val_accuracy: 0.6970\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.7242 - val_loss: 0.5157 - val_accuracy: 0.7391\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7586 - val_loss: 0.4653 - val_accuracy: 0.7602\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.7790 - val_loss: 0.4298 - val_accuracy: 0.7785\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8025 - val_loss: 0.4026 - val_accuracy: 0.7908\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8184 - val_loss: 0.3817 - val_accuracy: 0.8037\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8311 - val_loss: 0.3646 - val_accuracy: 0.8186\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3369 - accuracy: 0.8410 - val_loss: 0.3504 - val_accuracy: 0.8288\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8478 - val_loss: 0.3384 - val_accuracy: 0.8363\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3090 - accuracy: 0.8614 - val_loss: 0.3285 - val_accuracy: 0.8424\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8687 - val_loss: 0.3198 - val_accuracy: 0.8526\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8768 - val_loss: 0.3123 - val_accuracy: 0.8607\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8832 - val_loss: 0.3057 - val_accuracy: 0.8675\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.8877 - val_loss: 0.3000 - val_accuracy: 0.8764\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8909 - val_loss: 0.2947 - val_accuracy: 0.8832\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8963 - val_loss: 0.2901 - val_accuracy: 0.8879\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.8995 - val_loss: 0.2859 - val_accuracy: 0.8906\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9013 - val_loss: 0.2821 - val_accuracy: 0.8927\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2458 - accuracy: 0.9031 - val_loss: 0.2787 - val_accuracy: 0.8940\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9026 - val_loss: 0.2756 - val_accuracy: 0.8981\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.9040 - val_loss: 0.2726 - val_accuracy: 0.9001\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9049 - val_loss: 0.2700 - val_accuracy: 0.9035\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9062 - val_loss: 0.2675 - val_accuracy: 0.9062\n",
      "Epoch 24/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.9085 - val_loss: 0.2652 - val_accuracy: 0.9076\n",
      "Epoch 25/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9103 - val_loss: 0.2630 - val_accuracy: 0.9090\n",
      "Epoch 26/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9112 - val_loss: 0.2610 - val_accuracy: 0.9103\n",
      "Epoch 27/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2190 - accuracy: 0.9117 - val_loss: 0.2591 - val_accuracy: 0.9110\n",
      "Epoch 28/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9121 - val_loss: 0.2573 - val_accuracy: 0.9117\n",
      "Epoch 29/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2142 - accuracy: 0.9126 - val_loss: 0.2555 - val_accuracy: 0.9117\n",
      "Epoch 30/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9126 - val_loss: 0.2539 - val_accuracy: 0.9124\n",
      "Epoch 31/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9130 - val_loss: 0.2523 - val_accuracy: 0.9130\n",
      "Epoch 32/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2078 - accuracy: 0.9130 - val_loss: 0.2508 - val_accuracy: 0.9137\n",
      "Epoch 33/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9126 - val_loss: 0.2493 - val_accuracy: 0.9144\n",
      "Epoch 34/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9135 - val_loss: 0.2480 - val_accuracy: 0.9130\n",
      "Epoch 35/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2024 - accuracy: 0.9149 - val_loss: 0.2466 - val_accuracy: 0.9144\n",
      "Epoch 36/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9162 - val_loss: 0.2453 - val_accuracy: 0.9144\n",
      "Epoch 37/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.9153 - val_loss: 0.2443 - val_accuracy: 0.9137\n",
      "Epoch 38/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1977 - accuracy: 0.9167 - val_loss: 0.2431 - val_accuracy: 0.9144\n",
      "Epoch 39/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9185 - val_loss: 0.2421 - val_accuracy: 0.9144\n",
      "Epoch 40/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9189 - val_loss: 0.2411 - val_accuracy: 0.9137\n",
      "Epoch 41/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9189 - val_loss: 0.2401 - val_accuracy: 0.9144\n",
      "Epoch 42/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.9198 - val_loss: 0.2391 - val_accuracy: 0.9144\n",
      "Epoch 43/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9212 - val_loss: 0.2382 - val_accuracy: 0.9151\n",
      "Epoch 44/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9230 - val_loss: 0.2374 - val_accuracy: 0.9158\n",
      "Epoch 45/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9230 - val_loss: 0.2366 - val_accuracy: 0.9158\n",
      "Epoch 46/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1881 - accuracy: 0.9239 - val_loss: 0.2358 - val_accuracy: 0.9158\n",
      "Epoch 47/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9248 - val_loss: 0.2350 - val_accuracy: 0.9164\n",
      "Epoch 48/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9253 - val_loss: 0.2342 - val_accuracy: 0.9171\n",
      "Epoch 49/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9257 - val_loss: 0.2334 - val_accuracy: 0.9171\n",
      "Epoch 50/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9266 - val_loss: 0.2326 - val_accuracy: 0.9171\n",
      "Epoch 51/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9275 - val_loss: 0.2320 - val_accuracy: 0.9185\n",
      "Epoch 52/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.9271 - val_loss: 0.2313 - val_accuracy: 0.9185\n",
      "Epoch 53/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1812 - accuracy: 0.9293 - val_loss: 0.2306 - val_accuracy: 0.9185\n",
      "Epoch 54/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9293 - val_loss: 0.2300 - val_accuracy: 0.9185\n",
      "Epoch 55/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1796 - accuracy: 0.9293 - val_loss: 0.2294 - val_accuracy: 0.9185\n",
      "Epoch 56/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1787 - accuracy: 0.9298 - val_loss: 0.2288 - val_accuracy: 0.9198\n",
      "Epoch 57/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.9316 - val_loss: 0.2282 - val_accuracy: 0.9212\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9312 - val_loss: 0.2276 - val_accuracy: 0.9219\n",
      "Epoch 59/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9330 - val_loss: 0.2270 - val_accuracy: 0.9212\n",
      "Epoch 60/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1756 - accuracy: 0.9330 - val_loss: 0.2265 - val_accuracy: 0.9219\n",
      "Epoch 61/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1748 - accuracy: 0.9330 - val_loss: 0.2260 - val_accuracy: 0.9219\n",
      "Epoch 62/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1740 - accuracy: 0.9339 - val_loss: 0.2255 - val_accuracy: 0.9219\n",
      "Epoch 63/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1733 - accuracy: 0.9339 - val_loss: 0.2250 - val_accuracy: 0.9232\n",
      "Epoch 64/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1726 - accuracy: 0.9339 - val_loss: 0.2246 - val_accuracy: 0.9232\n",
      "Epoch 65/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9339 - val_loss: 0.2241 - val_accuracy: 0.9232\n",
      "Epoch 66/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.9343 - val_loss: 0.2236 - val_accuracy: 0.9239\n",
      "Epoch 67/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1706 - accuracy: 0.9348 - val_loss: 0.2231 - val_accuracy: 0.9239\n",
      "Epoch 68/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9343 - val_loss: 0.2227 - val_accuracy: 0.9239\n",
      "Epoch 69/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9348 - val_loss: 0.2224 - val_accuracy: 0.9253\n",
      "Epoch 70/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9357 - val_loss: 0.2220 - val_accuracy: 0.9260\n",
      "Epoch 71/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1681 - accuracy: 0.9357 - val_loss: 0.2216 - val_accuracy: 0.9266\n",
      "Epoch 72/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9352 - val_loss: 0.2212 - val_accuracy: 0.9266\n",
      "Epoch 73/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9361 - val_loss: 0.2208 - val_accuracy: 0.9266\n",
      "Epoch 74/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 0.9375 - val_loss: 0.2205 - val_accuracy: 0.9266\n",
      "Epoch 75/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.9375 - val_loss: 0.2201 - val_accuracy: 0.9266\n",
      "Epoch 76/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1654 - accuracy: 0.9375 - val_loss: 0.2197 - val_accuracy: 0.9266\n",
      "Epoch 77/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9370 - val_loss: 0.2194 - val_accuracy: 0.9260\n",
      "Epoch 78/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9375 - val_loss: 0.2190 - val_accuracy: 0.9260\n",
      "Epoch 79/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9375 - val_loss: 0.2187 - val_accuracy: 0.9260\n",
      "Epoch 80/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1633 - accuracy: 0.9380 - val_loss: 0.2184 - val_accuracy: 0.9260\n",
      "Epoch 81/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1628 - accuracy: 0.9370 - val_loss: 0.2181 - val_accuracy: 0.9253\n",
      "Epoch 82/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9370 - val_loss: 0.2179 - val_accuracy: 0.9253\n",
      "Epoch 83/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1618 - accuracy: 0.9384 - val_loss: 0.2176 - val_accuracy: 0.9246\n",
      "Epoch 84/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9384 - val_loss: 0.2174 - val_accuracy: 0.9260\n",
      "Epoch 85/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9375 - val_loss: 0.2171 - val_accuracy: 0.9260\n",
      "Epoch 86/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1604 - accuracy: 0.9393 - val_loss: 0.2168 - val_accuracy: 0.9253\n",
      "Epoch 87/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9384 - val_loss: 0.2165 - val_accuracy: 0.9253\n",
      "Epoch 88/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1595 - accuracy: 0.9380 - val_loss: 0.2162 - val_accuracy: 0.9260\n",
      "Epoch 89/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1591 - accuracy: 0.9384 - val_loss: 0.2160 - val_accuracy: 0.9266\n",
      "Epoch 90/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9384 - val_loss: 0.2158 - val_accuracy: 0.9260\n",
      "Epoch 91/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9389 - val_loss: 0.2155 - val_accuracy: 0.9260\n",
      "Epoch 92/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1578 - accuracy: 0.9393 - val_loss: 0.2152 - val_accuracy: 0.9260\n",
      "Epoch 93/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9389 - val_loss: 0.2150 - val_accuracy: 0.9266\n",
      "Epoch 94/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1570 - accuracy: 0.9398 - val_loss: 0.2147 - val_accuracy: 0.9266\n",
      "Epoch 95/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.9393 - val_loss: 0.2146 - val_accuracy: 0.9266\n",
      "Epoch 96/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9402 - val_loss: 0.2142 - val_accuracy: 0.9266\n",
      "Epoch 97/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1558 - accuracy: 0.9389 - val_loss: 0.2140 - val_accuracy: 0.9266\n",
      "Epoch 98/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1554 - accuracy: 0.9402 - val_loss: 0.2138 - val_accuracy: 0.9266\n",
      "Epoch 99/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9393 - val_loss: 0.2137 - val_accuracy: 0.9266\n",
      "Epoch 100/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1547 - accuracy: 0.9402 - val_loss: 0.2134 - val_accuracy: 0.9266\n"
     ]
    }
   ],
   "source": [
    "## Training the model\n",
    "train_history = model10.fit(train_dataset, epochs = 100, validation_data= val_dataset )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 10 neurons case\n",
    "loss = train_history.history['loss']\n",
    "val_loss10 = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss10)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 69 steps, validate for 46 steps\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.6204 - accuracy: 0.6544 - val_loss: 0.5274 - val_accuracy: 0.7072\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7477 - val_loss: 0.4444 - val_accuracy: 0.7656\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.7953 - val_loss: 0.3982 - val_accuracy: 0.7935\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8225 - val_loss: 0.3678 - val_accuracy: 0.8118\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8428 - val_loss: 0.3464 - val_accuracy: 0.8227\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8514 - val_loss: 0.3308 - val_accuracy: 0.8308\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8614 - val_loss: 0.3187 - val_accuracy: 0.8438\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8664 - val_loss: 0.3092 - val_accuracy: 0.8512\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.8745 - val_loss: 0.3014 - val_accuracy: 0.8580\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.8886 - val_loss: 0.2950 - val_accuracy: 0.8689\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.8913 - val_loss: 0.2894 - val_accuracy: 0.8750\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.8967 - val_loss: 0.2847 - val_accuracy: 0.8784\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.8976 - val_loss: 0.2805 - val_accuracy: 0.8804\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.9004 - val_loss: 0.2769 - val_accuracy: 0.8811\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.9040 - val_loss: 0.2736 - val_accuracy: 0.8879\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9035 - val_loss: 0.2706 - val_accuracy: 0.8913\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9058 - val_loss: 0.2678 - val_accuracy: 0.8920\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9085 - val_loss: 0.2653 - val_accuracy: 0.8927\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9090 - val_loss: 0.2631 - val_accuracy: 0.8927\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9126 - val_loss: 0.2610 - val_accuracy: 0.8954\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.9139 - val_loss: 0.2591 - val_accuracy: 0.8974\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9149 - val_loss: 0.2572 - val_accuracy: 0.8974\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2077 - accuracy: 0.9144 - val_loss: 0.2554 - val_accuracy: 0.8974\n",
      "Epoch 24/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9162 - val_loss: 0.2537 - val_accuracy: 0.8974\n",
      "Epoch 25/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9158 - val_loss: 0.2520 - val_accuracy: 0.8995\n",
      "Epoch 26/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9162 - val_loss: 0.2505 - val_accuracy: 0.9035\n",
      "Epoch 27/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1986 - accuracy: 0.9171 - val_loss: 0.2490 - val_accuracy: 0.9035\n",
      "Epoch 28/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1967 - accuracy: 0.9180 - val_loss: 0.2476 - val_accuracy: 0.9049\n",
      "Epoch 29/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1948 - accuracy: 0.9189 - val_loss: 0.2462 - val_accuracy: 0.9056\n",
      "Epoch 30/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1930 - accuracy: 0.9203 - val_loss: 0.2449 - val_accuracy: 0.9056\n",
      "Epoch 31/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9203 - val_loss: 0.2438 - val_accuracy: 0.9056\n",
      "Epoch 32/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.9212 - val_loss: 0.2426 - val_accuracy: 0.9069\n",
      "Epoch 33/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9226 - val_loss: 0.2413 - val_accuracy: 0.9069\n",
      "Epoch 34/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1863 - accuracy: 0.9226 - val_loss: 0.2402 - val_accuracy: 0.9076\n",
      "Epoch 35/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.9244 - val_loss: 0.2392 - val_accuracy: 0.9110\n",
      "Epoch 36/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9239 - val_loss: 0.2381 - val_accuracy: 0.9110\n",
      "Epoch 37/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1820 - accuracy: 0.9262 - val_loss: 0.2371 - val_accuracy: 0.9144\n",
      "Epoch 38/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1805 - accuracy: 0.9271 - val_loss: 0.2362 - val_accuracy: 0.9151\n",
      "Epoch 39/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1793 - accuracy: 0.9271 - val_loss: 0.2352 - val_accuracy: 0.9164\n",
      "Epoch 40/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1780 - accuracy: 0.9280 - val_loss: 0.2344 - val_accuracy: 0.9164\n",
      "Epoch 41/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9289 - val_loss: 0.2335 - val_accuracy: 0.9158\n",
      "Epoch 42/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.9284 - val_loss: 0.2326 - val_accuracy: 0.9151\n",
      "Epoch 43/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9289 - val_loss: 0.2318 - val_accuracy: 0.9151\n",
      "Epoch 44/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9312 - val_loss: 0.2310 - val_accuracy: 0.9151\n",
      "Epoch 45/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9316 - val_loss: 0.2303 - val_accuracy: 0.9151\n",
      "Epoch 46/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9316 - val_loss: 0.2294 - val_accuracy: 0.9151\n",
      "Epoch 47/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1697 - accuracy: 0.9325 - val_loss: 0.2287 - val_accuracy: 0.9158\n",
      "Epoch 48/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1686 - accuracy: 0.9352 - val_loss: 0.2279 - val_accuracy: 0.9164\n",
      "Epoch 49/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9339 - val_loss: 0.2271 - val_accuracy: 0.9171\n",
      "Epoch 50/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 0.9339 - val_loss: 0.2264 - val_accuracy: 0.9178\n",
      "Epoch 51/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1656 - accuracy: 0.9348 - val_loss: 0.2257 - val_accuracy: 0.9185\n",
      "Epoch 52/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1645 - accuracy: 0.9361 - val_loss: 0.2250 - val_accuracy: 0.9192\n",
      "Epoch 53/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1636 - accuracy: 0.9352 - val_loss: 0.2244 - val_accuracy: 0.9219\n",
      "Epoch 54/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.9361 - val_loss: 0.2238 - val_accuracy: 0.9226\n",
      "Epoch 55/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1617 - accuracy: 0.9361 - val_loss: 0.2231 - val_accuracy: 0.9226\n",
      "Epoch 56/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1608 - accuracy: 0.9366 - val_loss: 0.2224 - val_accuracy: 0.9226\n",
      "Epoch 57/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9366 - val_loss: 0.2218 - val_accuracy: 0.9226\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1591 - accuracy: 0.9366 - val_loss: 0.2212 - val_accuracy: 0.9226\n",
      "Epoch 59/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9370 - val_loss: 0.2206 - val_accuracy: 0.9232\n",
      "Epoch 60/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1573 - accuracy: 0.9389 - val_loss: 0.2199 - val_accuracy: 0.9226\n",
      "Epoch 61/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9375 - val_loss: 0.2193 - val_accuracy: 0.9232\n",
      "Epoch 62/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9380 - val_loss: 0.2188 - val_accuracy: 0.9239\n",
      "Epoch 63/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1548 - accuracy: 0.9384 - val_loss: 0.2182 - val_accuracy: 0.9239\n",
      "Epoch 64/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9398 - val_loss: 0.2177 - val_accuracy: 0.9246\n",
      "Epoch 65/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9389 - val_loss: 0.2172 - val_accuracy: 0.9239\n",
      "Epoch 66/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1525 - accuracy: 0.9402 - val_loss: 0.2167 - val_accuracy: 0.9253\n",
      "Epoch 67/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9411 - val_loss: 0.2162 - val_accuracy: 0.9253\n",
      "Epoch 68/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1509 - accuracy: 0.9416 - val_loss: 0.2157 - val_accuracy: 0.9273\n",
      "Epoch 69/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.9416 - val_loss: 0.2152 - val_accuracy: 0.9273\n",
      "Epoch 70/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1495 - accuracy: 0.9416 - val_loss: 0.2147 - val_accuracy: 0.9260\n",
      "Epoch 71/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1488 - accuracy: 0.9420 - val_loss: 0.2142 - val_accuracy: 0.9266\n",
      "Epoch 72/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1481 - accuracy: 0.9420 - val_loss: 0.2138 - val_accuracy: 0.9273\n",
      "Epoch 73/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.9425 - val_loss: 0.2133 - val_accuracy: 0.9273\n",
      "Epoch 74/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.9425 - val_loss: 0.2128 - val_accuracy: 0.9280\n",
      "Epoch 75/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1461 - accuracy: 0.9438 - val_loss: 0.2125 - val_accuracy: 0.9280\n",
      "Epoch 76/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9434 - val_loss: 0.2121 - val_accuracy: 0.9287\n",
      "Epoch 77/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9438 - val_loss: 0.2117 - val_accuracy: 0.9287\n",
      "Epoch 78/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9443 - val_loss: 0.2112 - val_accuracy: 0.9287\n",
      "Epoch 79/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9438 - val_loss: 0.2109 - val_accuracy: 0.9300\n",
      "Epoch 80/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9443 - val_loss: 0.2105 - val_accuracy: 0.9307\n",
      "Epoch 81/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9447 - val_loss: 0.2101 - val_accuracy: 0.9307\n",
      "Epoch 82/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9452 - val_loss: 0.2097 - val_accuracy: 0.9307\n",
      "Epoch 83/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1412 - accuracy: 0.9447 - val_loss: 0.2094 - val_accuracy: 0.9307\n",
      "Epoch 84/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1407 - accuracy: 0.9452 - val_loss: 0.2091 - val_accuracy: 0.9307\n",
      "Epoch 85/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1401 - accuracy: 0.9457 - val_loss: 0.2086 - val_accuracy: 0.9300\n",
      "Epoch 86/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1395 - accuracy: 0.9457 - val_loss: 0.2083 - val_accuracy: 0.9300\n",
      "Epoch 87/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9457 - val_loss: 0.2079 - val_accuracy: 0.9293\n",
      "Epoch 88/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9461 - val_loss: 0.2076 - val_accuracy: 0.9293\n",
      "Epoch 89/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.9461 - val_loss: 0.2072 - val_accuracy: 0.9293\n",
      "Epoch 90/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9461 - val_loss: 0.2069 - val_accuracy: 0.9293\n",
      "Epoch 91/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9461 - val_loss: 0.2066 - val_accuracy: 0.9293\n",
      "Epoch 92/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9466 - val_loss: 0.2063 - val_accuracy: 0.9293\n",
      "Epoch 93/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9466 - val_loss: 0.2060 - val_accuracy: 0.9287\n",
      "Epoch 94/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9475 - val_loss: 0.2057 - val_accuracy: 0.9287\n",
      "Epoch 95/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9470 - val_loss: 0.2054 - val_accuracy: 0.9287\n",
      "Epoch 96/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.9484 - val_loss: 0.2050 - val_accuracy: 0.9287\n",
      "Epoch 97/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1337 - accuracy: 0.9475 - val_loss: 0.2047 - val_accuracy: 0.9280\n",
      "Epoch 98/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1333 - accuracy: 0.9475 - val_loss: 0.2044 - val_accuracy: 0.9287\n",
      "Epoch 99/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9479 - val_loss: 0.2041 - val_accuracy: 0.9293\n",
      "Epoch 100/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9484 - val_loss: 0.2039 - val_accuracy: 0.9293\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 100 neurons case\n",
    "model100 = get_compiled_model100()\n",
    "train_history = model100.fit(train_dataset, epochs = 100, validation_data= val_dataset )\n",
    "loss = train_history.history['loss']\n",
    "val_loss100 = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss100)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 69 steps, validate for 46 steps\n",
      "Epoch 1/100\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 0.5775 - accuracy: 0.6667 - val_loss: 0.5026 - val_accuracy: 0.7038\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.7736 - val_loss: 0.4215 - val_accuracy: 0.7785\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8184 - val_loss: 0.3776 - val_accuracy: 0.8030\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8401 - val_loss: 0.3504 - val_accuracy: 0.8166\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8542 - val_loss: 0.3319 - val_accuracy: 0.8261\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8596 - val_loss: 0.3181 - val_accuracy: 0.8404\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.8700 - val_loss: 0.3077 - val_accuracy: 0.8512\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8764 - val_loss: 0.2995 - val_accuracy: 0.8573\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8836 - val_loss: 0.2927 - val_accuracy: 0.8594\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8890 - val_loss: 0.2871 - val_accuracy: 0.8675\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.8909 - val_loss: 0.2822 - val_accuracy: 0.8723\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2444 - accuracy: 0.8940 - val_loss: 0.2779 - val_accuracy: 0.8764\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.8963 - val_loss: 0.2743 - val_accuracy: 0.8804\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2350 - accuracy: 0.8981 - val_loss: 0.2710 - val_accuracy: 0.8825\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.9013 - val_loss: 0.2679 - val_accuracy: 0.8832\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9040 - val_loss: 0.2652 - val_accuracy: 0.8865\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9058 - val_loss: 0.2627 - val_accuracy: 0.8886\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9062 - val_loss: 0.2604 - val_accuracy: 0.8893\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9081 - val_loss: 0.2582 - val_accuracy: 0.8913\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9081 - val_loss: 0.2563 - val_accuracy: 0.8920\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.9108 - val_loss: 0.2544 - val_accuracy: 0.8933\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9112 - val_loss: 0.2526 - val_accuracy: 0.8988\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9126 - val_loss: 0.2509 - val_accuracy: 0.9022\n",
      "Epoch 24/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.9117 - val_loss: 0.2492 - val_accuracy: 0.9029\n",
      "Epoch 25/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9126 - val_loss: 0.2477 - val_accuracy: 0.9035\n",
      "Epoch 26/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9126 - val_loss: 0.2463 - val_accuracy: 0.9042\n",
      "Epoch 27/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9144 - val_loss: 0.2449 - val_accuracy: 0.9062\n",
      "Epoch 28/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9167 - val_loss: 0.2435 - val_accuracy: 0.9076\n",
      "Epoch 29/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9171 - val_loss: 0.2422 - val_accuracy: 0.9083\n",
      "Epoch 30/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9180 - val_loss: 0.2410 - val_accuracy: 0.9090\n",
      "Epoch 31/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9180 - val_loss: 0.2398 - val_accuracy: 0.9090\n",
      "Epoch 32/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1890 - accuracy: 0.9203 - val_loss: 0.2387 - val_accuracy: 0.9103\n",
      "Epoch 33/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9221 - val_loss: 0.2376 - val_accuracy: 0.9110\n",
      "Epoch 34/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1859 - accuracy: 0.9226 - val_loss: 0.2365 - val_accuracy: 0.9110\n",
      "Epoch 35/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.9235 - val_loss: 0.2355 - val_accuracy: 0.9117\n",
      "Epoch 36/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1829 - accuracy: 0.9248 - val_loss: 0.2345 - val_accuracy: 0.9137\n",
      "Epoch 37/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9266 - val_loss: 0.2335 - val_accuracy: 0.9137\n",
      "Epoch 38/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1801 - accuracy: 0.9257 - val_loss: 0.2325 - val_accuracy: 0.9130\n",
      "Epoch 39/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9266 - val_loss: 0.2317 - val_accuracy: 0.9158\n",
      "Epoch 40/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9293 - val_loss: 0.2308 - val_accuracy: 0.9171\n",
      "Epoch 41/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9298 - val_loss: 0.2299 - val_accuracy: 0.9185\n",
      "Epoch 42/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.9307 - val_loss: 0.2290 - val_accuracy: 0.9185\n",
      "Epoch 43/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9321 - val_loss: 0.2283 - val_accuracy: 0.9192\n",
      "Epoch 44/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9307 - val_loss: 0.2275 - val_accuracy: 0.9198\n",
      "Epoch 45/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1713 - accuracy: 0.9316 - val_loss: 0.2268 - val_accuracy: 0.9198\n",
      "Epoch 46/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9330 - val_loss: 0.2260 - val_accuracy: 0.9198\n",
      "Epoch 47/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1690 - accuracy: 0.9325 - val_loss: 0.2254 - val_accuracy: 0.9198\n",
      "Epoch 48/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1679 - accuracy: 0.9343 - val_loss: 0.2247 - val_accuracy: 0.9212\n",
      "Epoch 49/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1668 - accuracy: 0.9366 - val_loss: 0.2239 - val_accuracy: 0.9212\n",
      "Epoch 50/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1657 - accuracy: 0.9361 - val_loss: 0.2234 - val_accuracy: 0.9205\n",
      "Epoch 51/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9361 - val_loss: 0.2228 - val_accuracy: 0.9212\n",
      "Epoch 52/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1637 - accuracy: 0.9370 - val_loss: 0.2222 - val_accuracy: 0.9212\n",
      "Epoch 53/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.9389 - val_loss: 0.2215 - val_accuracy: 0.9226\n",
      "Epoch 54/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1617 - accuracy: 0.9389 - val_loss: 0.2209 - val_accuracy: 0.9219\n",
      "Epoch 55/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.9384 - val_loss: 0.2203 - val_accuracy: 0.9226\n",
      "Epoch 56/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9384 - val_loss: 0.2199 - val_accuracy: 0.9226\n",
      "Epoch 57/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1589 - accuracy: 0.9398 - val_loss: 0.2194 - val_accuracy: 0.9219\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1579 - accuracy: 0.9398 - val_loss: 0.2188 - val_accuracy: 0.9219\n",
      "Epoch 59/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1570 - accuracy: 0.9402 - val_loss: 0.2182 - val_accuracy: 0.9219\n",
      "Epoch 60/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1561 - accuracy: 0.9402 - val_loss: 0.2177 - val_accuracy: 0.9219\n",
      "Epoch 61/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1553 - accuracy: 0.9407 - val_loss: 0.2172 - val_accuracy: 0.9226\n",
      "Epoch 62/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9407 - val_loss: 0.2167 - val_accuracy: 0.9226\n",
      "Epoch 63/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1535 - accuracy: 0.9411 - val_loss: 0.2161 - val_accuracy: 0.9219\n",
      "Epoch 64/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1527 - accuracy: 0.9407 - val_loss: 0.2157 - val_accuracy: 0.9232\n",
      "Epoch 65/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9425 - val_loss: 0.2153 - val_accuracy: 0.9232\n",
      "Epoch 66/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9425 - val_loss: 0.2148 - val_accuracy: 0.9246\n",
      "Epoch 67/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9416 - val_loss: 0.2143 - val_accuracy: 0.9232\n",
      "Epoch 68/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1495 - accuracy: 0.9425 - val_loss: 0.2139 - val_accuracy: 0.9246\n",
      "Epoch 69/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9420 - val_loss: 0.2135 - val_accuracy: 0.9246\n",
      "Epoch 70/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9416 - val_loss: 0.2131 - val_accuracy: 0.9253\n",
      "Epoch 71/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9416 - val_loss: 0.2126 - val_accuracy: 0.9246\n",
      "Epoch 72/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9420 - val_loss: 0.2122 - val_accuracy: 0.9246\n",
      "Epoch 73/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9434 - val_loss: 0.2117 - val_accuracy: 0.9246\n",
      "Epoch 74/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9429 - val_loss: 0.2114 - val_accuracy: 0.9253\n",
      "Epoch 75/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9425 - val_loss: 0.2111 - val_accuracy: 0.9253\n",
      "Epoch 76/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9425 - val_loss: 0.2107 - val_accuracy: 0.9253\n",
      "Epoch 77/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9434 - val_loss: 0.2103 - val_accuracy: 0.9253\n",
      "Epoch 78/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9438 - val_loss: 0.2100 - val_accuracy: 0.9260\n",
      "Epoch 79/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9438 - val_loss: 0.2096 - val_accuracy: 0.9260\n",
      "Epoch 80/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9438 - val_loss: 0.2092 - val_accuracy: 0.9260\n",
      "Epoch 81/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9443 - val_loss: 0.2089 - val_accuracy: 0.9260\n",
      "Epoch 82/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1396 - accuracy: 0.9443 - val_loss: 0.2085 - val_accuracy: 0.9266\n",
      "Epoch 83/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9443 - val_loss: 0.2082 - val_accuracy: 0.9273\n",
      "Epoch 84/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.9447 - val_loss: 0.2078 - val_accuracy: 0.9266\n",
      "Epoch 85/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1376 - accuracy: 0.9434 - val_loss: 0.2074 - val_accuracy: 0.9273\n",
      "Epoch 86/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9443 - val_loss: 0.2071 - val_accuracy: 0.9273\n",
      "Epoch 87/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9452 - val_loss: 0.2068 - val_accuracy: 0.9273\n",
      "Epoch 88/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9443 - val_loss: 0.2064 - val_accuracy: 0.9273\n",
      "Epoch 89/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9447 - val_loss: 0.2061 - val_accuracy: 0.9273\n",
      "Epoch 90/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9452 - val_loss: 0.2057 - val_accuracy: 0.9273\n",
      "Epoch 91/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9452 - val_loss: 0.2054 - val_accuracy: 0.9280\n",
      "Epoch 92/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9452 - val_loss: 0.2051 - val_accuracy: 0.9280\n",
      "Epoch 93/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9461 - val_loss: 0.2047 - val_accuracy: 0.9280\n",
      "Epoch 94/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9461 - val_loss: 0.2044 - val_accuracy: 0.9280\n",
      "Epoch 95/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9457 - val_loss: 0.2042 - val_accuracy: 0.9287\n",
      "Epoch 96/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1312 - accuracy: 0.9470 - val_loss: 0.2040 - val_accuracy: 0.9287\n",
      "Epoch 97/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9475 - val_loss: 0.2037 - val_accuracy: 0.9287\n",
      "Epoch 98/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9470 - val_loss: 0.2035 - val_accuracy: 0.9300\n",
      "Epoch 99/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1296 - accuracy: 0.9475 - val_loss: 0.2031 - val_accuracy: 0.9307\n",
      "Epoch 100/100\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9484 - val_loss: 0.2029 - val_accuracy: 0.9307\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 1000 neurons case\n",
    "model1000 = get_compiled_model1000()\n",
    "train_history = model1000.fit(train_dataset, epochs = 100, validation_data= val_dataset )\n",
    "loss = train_history.history['loss']\n",
    "val_loss1000 = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss1000)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "99\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "### The best epoch with minimized validation \n",
    "print(np.argmin(val_loss10))\n",
    "print(np.argmin(val_loss100))\n",
    "print(np.argmin(val_loss1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921/921 - 0s - loss: 0.1773 - accuracy: 0.9251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17727952752258308, 0.92508143]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prediction Accuracy on test data set\n",
    "model10.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921/921 - 0s - loss: 0.1592 - accuracy: 0.9338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15918787206390653, 0.9337676]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model100.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921/921 - 0s - loss: 0.1629 - accuracy: 0.9273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16294475457955648, 0.927253]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1000.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
